# -*- coding: utf-8 -*-
"""Interactive chat script for a finetuned gpt-oss-20b / Qwen3 model (FSDP checkpoints)."""

import os
import sys
import json
import argparse
import warnings
from pathlib import Path

import torch
from loguru import logger
import transformers
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    Qwen3Config,
    Qwen3ForCausalLM,
)

# silence HF warnings
warnings.filterwarnings("ignore")
transformers.logging.set_verbosity_error()

# ------------------------------------------------------------
# SDPA äº’æ›ãƒ©ãƒƒãƒ‘ï¼ˆPyTorch 2.4+ / ãã‚Œæœªæº€ ä¸¡å¯¾å¿œï¼‰
# ------------------------------------------------------------
def _build_sdpa_ctx():
    try:
        from torch.nn.attention import sdpa_kernel as _new_sdpa_kernel, SDPBackend
        def _ctx():
            return _new_sdpa_kernel(
                backends=[
                    SDPBackend.FLASH_ATTENTION,
                    SDPBackend.EFFICIENT_ATTENTION,
                    SDPBackend.MATH,
                ]
            )
        return _ctx
    except Exception:
        def _ctx():
            return torch.backends.cuda.sdp_kernel(
                enable_flash=True, enable_mem_efficient=True, enable_math=True
            )
        return _ctx

sdpa_ctx = _build_sdpa_ctx()

# ------------------------------------------------------------
# Paths (defaults)
# ------------------------------------------------------------
DATA_PATH = Path(os.getcwd())
CKPT_DIR = DATA_PATH / "checkpoints" / "last"
CKPT_PATH = CKPT_DIR / "final_checkpoint.pth"          # FSDP æœ€çµ‚
ALT_PT    = DATA_PATH / "checkpoints" / "pytorch_model.pt"  # ä¸­é–“
CONFIG_PATH = CKPT_DIR / "config.json"                 # å­¦ç¿’å´ save_config ã®ã‚‚ã®

SIZE_LOOKUP = {1024: "0.6B", 2048: "1.7B", 2560: "4B", 2880: "20B", 4096: "8B"}

def infer_size_from_state_dict(sd: dict) -> str:
    for k, w in sd.items():
        if isinstance(w, torch.Tensor) and w.ndim == 2:
            hidden = w.shape[-1]
            if hidden in SIZE_LOOKUP:
                return SIZE_LOOKUP[hidden]
    raise RuntimeError("Could not infer model size from checkpoint tensors.")

def _load_ckpt(path: Path) -> dict:
    if not path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {path}")
    ckpt = torch.load(path, map_location="cpu")
    if "model_state_dict" not in ckpt:
        # äº’æ›ï¼šã¾ã‚Œã«ãã®ã¾ã¾ state_dict ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆ
        if all(isinstance(v, torch.Tensor) for v in ckpt.values()):
            ckpt = {"model_state_dict": ckpt}
        else:
            raise RuntimeError(f"Invalid checkpoint format: keys={list(ckpt.keys())[:5]}")
    return ckpt

def _read_training_config(config_path: Path) -> dict:
    if not config_path.exists():
        return {}
    try:
        obj = json.loads(config_path.read_text())
        # å­¦ç¿’å´ save_config ã¯ {"model_name": "...", "tokenizer_config": {...}}
        return obj if isinstance(obj, dict) else {}
    except Exception:
        return {}

def _build_model_from_model_name(model_name: str):
    """
    ãƒ¢ãƒ‡ãƒ«ã®å…¨é‡ã¿ã¯DLã›ãšã€AutoConfigâ†’from_config ã§ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã ã‘æ§‹ç¯‰ã€‚
    """
    logger.info(f"Building model from config of [{model_name}] (no base weights download)")
    cfg = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_config(cfg, trust_remote_code=True)
    tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)
    return model, tok

def _fallback_build_qwen3(size_label: str):
    # æœ€çµ‚æ‰‹æ®µï¼ˆãƒãƒƒãƒˆä¸å¯ç­‰ã§ AutoConfig å–ã‚Œãªã„æ™‚ï¼‰
    logger.warning(f"Falling back to Qwen3 skeleton: size={size_label}")
    qcfg = Qwen3Config()  # æœ€ä½é™ã€‚å¿…è¦ãªã‚‰ç‹¬è‡ªã® BaseModel å®Ÿè£…ã«å·®ã—æ›¿ãˆã¦ãã ã•ã„ã€‚
    model = Qwen3ForCausalLM(qcfg)
    tok = AutoTokenizer.from_pretrained(f"Qwen/Qwen3-{size_label}", trust_remote_code=True)
    return model, tok

def load_model(checkpoint_path: Path, config_path: Path, dtype: torch.dtype = torch.bfloat16):
    ckpt = _load_ckpt(checkpoint_path)
    train_cfg = _read_training_config(config_path)

    model_name = train_cfg.get("model_name", "") or train_cfg.get("_name_or_path", "")
    model = None
    tok = None

    if model_name:
        try:
            model, tok = _build_model_from_model_name(model_name)
        except Exception as e:
            logger.warning(f"Failed to build from model_name={model_name}: {e}")

    if model is None:
        # config.json ãŒç„¡ã„ï¼å£Šã‚Œã¦ã„ã‚‹æ™‚ã¯ã‚µã‚¤ã‚ºæ¨å®š â†’ Qwen3 éª¨çµ„ã¿
        size = infer_size_from_state_dict(ckpt["model_state_dict"])
        model, tok = _fallback_build_qwen3(size)

    # é‡ã¿ãƒ­ãƒ¼ãƒ‰ï¼ˆFSDP FULL_STATE ã‚’æƒ³å®šï¼‰
    missing, unexpected = model.load_state_dict(ckpt["model_state_dict"], strict=False)
    if missing:
        logger.warning(f"Missing keys: {len(missing)} (showing up to 10): {missing[:10]}")
    if unexpected:
        logger.warning(f"Unexpected keys: {len(unexpected)} (showing up to 10): {unexpected[:10]}")

    # dtype & device æœªè¨­å®šï¼ˆå¾Œã§ .to(device, dtype) ã™ã‚‹ï¼‰
    if getattr(model.config, "pad_token_id", None) is None and getattr(model.config, "eos_token_id", None) is not None:
        model.config.pad_token_id = model.config.eos_token_id

    # ã‚µã‚¤ã‚ºè¡¨è¨˜ï¼ˆæ¨å®šï¼‰
    try:
        any_w = next(t for t in model.state_dict().values() if t.ndim == 2)
        size_label = SIZE_LOOKUP.get(any_w.shape[-1], "unknown")
    except Exception:
        size_label = "unknown"

    return model, tok, size_label

def main():
    p = argparse.ArgumentParser()
    p.add_argument("--checkpoint", default=str(CKPT_PATH),
                   help="FSDPã§ä¿å­˜ã—ãŸ .pth / .pt ã®ãƒ‘ã‚¹ï¼ˆfinal ã§ã‚‚ä¸­é–“ã§ã‚‚OKï¼‰")
    p.add_argument("--config", default=str(CONFIG_PATH),
                   help="å­¦ç¿’å´ save_config() ãŒå‡ºåŠ›ã—ãŸ config.json ã®ãƒ‘ã‚¹")
    p.add_argument("--max_new_tokens", type=int, default=512)
    p.add_argument("--temperature", type=float, default=0.7)
    p.add_argument("--top_p", type=float, default=0.9)
    p.add_argument("--top_k", type=int, default=50)
    p.add_argument("--dtype", default="bf16", choices=["bf16", "fp16", "fp32"])
    args = p.parse_args()

    dtype_map = {"bf16": torch.bfloat16, "fp16": torch.float16, "fp32": torch.float32}
    dtype = dtype_map[args.dtype]

    model, tokenizer, sz = load_model(Path(args.checkpoint), Path(args.config), dtype=dtype)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device, dtype=dtype)
    model.eval()

    # sdpa æœ‰åŠ¹åŒ–
    print(f"\nğŸš€ Chat ready â€” model size **{sz}**, dtype={dtype}, device={device}. Type [exit] to quit.\n")
    if device.type == "cuda":
        print("Using SDPA attention backends.\n")

    # stdin: ensure UTF-8
    if hasattr(sys.stdin, "reconfigure"):
        sys.stdin.reconfigure(encoding="utf-8")

    gen_kwargs = dict(
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        top_k=args.top_k,
        top_p=args.top_p,
        do_sample=True,
        pad_token_id=model.config.pad_token_id,
        eos_token_id=model.config.eos_token_id,
    )

    with sdpa_ctx():
        while True:
            try:
                usr = input("ğŸ‘¤ You: ")
                if usr.strip().lower() == "[exit]":
                    print("ğŸ‘‹ Bye!"); break

                inputs = tokenizer(usr, return_tensors="pt").to(device)
                with torch.inference_mode(), torch.autocast(device_type="cuda", dtype=dtype if dtype != torch.float32 else None):
                    out = model.generate(**inputs, **gen_kwargs)

                # å…ˆé ­ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®‰å…¨ã«å–ã‚Šé™¤ã
                gen = out[0][inputs["input_ids"].shape[1]:]
                ans = tokenizer.decode(gen, skip_special_tokens=True)
                print(f"ğŸ¤– Akabeko: {ans}\n")

            except KeyboardInterrupt:
                print("\nInterrupted. Type [exit] to quit.")
            except UnicodeDecodeError as e:
                print(f"Unicode error: {e}")
            except Exception as e:
                print(f"Error: {e}")

if __name__ == "__main__":
    main()
