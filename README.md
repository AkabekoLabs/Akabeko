# AkabekoLLM

## AkabekoLLMとは？

小さいモデルをベースにした事前学習や、継続事前学習を目的としたプロジェクトです。　

## AkabekoLLMでできること

torchrunを使って分散GPUで、Qwen3ベースのモデルの事前学習や継続事前学習が可能です。

## 学習時間(gpt-oss-20b)

- DeepSpeedに対応
- H200ベースで検証中

## 学習時間(Qwen)

事前学習の学習時間(1Bトークン)

|学習方法|モデル|モデル規模|トークン規模|Optimizer|2xH100 SXM|4xH100 SXM|8xH100 SXM|バッチサイズ|Storage容量|
|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|
|事前学習|Qwen3型|0.6B|1B|adamw||11分12秒|4分35秒|16|<100GB|
|事前学習|Qwen3型|1.7B|1B|adamw|||5分11秒|16|<100GB|
|事前学習|Qwen3型|4B|1B|adamw|||11分48秒|8|<100GB|
|事前学習|Qwen3型|0.6B|1B|muon||14分10秒|6分6秒|16|<100GB|

継続事前学習の学習時間(1Bトークン)

|学習方法|モデル|モデル規模|トークン規模|Optimizer|2xH100 SXM|4xH100 SXM|8xH100 SXM|バッチサイズ|Storage容量|
|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|
|継続事前学習|Qwen3-0.6B|0.6B|1B|adamw||7分56秒|3分27秒|16|<100GB|
|継続事前学習|Qwen3-1.7B|1.7B|1B|adamw|||5分12秒|16|<100GB|
|継続事前学習|Qwen3-4B|4B|1B|adamw|||11分51秒|8|<100GB|
|継続事前学習|Qwen3-8B|8B|1B|adamw|||34分33秒|2|>150GB|


## 各種使い方

Finewebから1Bトークンのデータセットを作成

```
make_data_fineweb_1b.sh
```

2GPUを用いて、Qweb3-0.6Bで継続事前学習

```
./2gpu_continue.sh
```

4GPUを用いて、Qweb3-0.6Bで継続事前学習

```
./4gpu_continue.sh
```

## 今後の拡張

各種学習方法手法の安定化を目指す
