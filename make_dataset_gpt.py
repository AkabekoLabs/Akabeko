import json
import os
import argparse
import csv
import io
import torch
from datasets import load_dataset
from transformers import AutoTokenizer

MODEL_NAME = "openai/gpt-oss-20b"
DEFAULT_OUTPUT_DIR = "tokenized_dataset"
DEFAULT_TARGET_TOKENS = 100_000_000  # æ—¢å®šå€¤ã¯å¾“æ¥ã©ãŠã‚Š

# è³ªå•/å›ç­”ã‚«ãƒ©ãƒ åã®å€™è£œï¼ˆæ—¥æœ¬èªãƒ»è‹±èªãƒ»ç•¥ç§°ãªã©ï¼‰
QUESTION_FIELD_CANDIDATES = [
    "Question", "question", "è³ªå•", "å•ã„", "ãŠé¡Œ", "Q", "user", "prompt", "instruction"
]
ANSWER_FIELD_CANDIDATES = [
    "Answer", "answer", "å›ç­”", "è§£ç­”", "A", "assistant", "output", "completion"
]

def sanitize_filename(name: str) -> str:
    return name.replace("/", "__").replace("\\", "__")

def iter_hf_examples(dataset_name: str):
    # HF: instruction / input / output å½¢å¼ã‚’æƒ³å®š
    ds = load_dataset(dataset_name, split="train", streaming=True)
    for ex in ds:
        instruction = (ex.get("instruction") or "").strip()
        input_text  = (ex.get("input") or "").strip()
        output_text = (ex.get("output") or "").strip()
        if not instruction or not output_text:
            continue
        user_message = f"{instruction}\n{input_text}" if input_text else instruction
        yield {
            "user": user_message,
            "assistant": output_text
        }

def sniff_csv(file_path: str, encoding: str, sample_bytes: int = 8192):
    """
    åŒºåˆ‡ã‚Šæ–‡å­—ã¨ãƒ˜ãƒƒãƒ€ãƒ¼æœ‰ç„¡ã‚’è‡ªå‹•æ¨å®š
    """
    with open(file_path, "rb") as rb:
        raw = rb.read(sample_bytes)
    # BOM ã‚„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å·®ç•°ã«å¼·ã„ã‚ˆã†ã«
    sample = raw.decode(encoding, errors="replace")
    sniffer = csv.Sniffer()
    try:
        dialect = sniffer.sniff(sample, delimiters=[",", "\t", ";", "|"])
        delimiter = dialect.delimiter
    except Exception:
        delimiter = ","  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    try:
        has_header = sniffer.has_header(sample)
    except Exception:
        has_header = True
    return delimiter, has_header

def pick_field_by_candidates(fieldnames, candidates):
    for cand in candidates:
        if cand in fieldnames:
            return cand
    # ã‚†ã‚‹ã„ä¸€è‡´ï¼ˆå°æ–‡å­—åŒ–ï¼‰
    lower = {f.lower(): f for f in fieldnames}
    for cand in candidates:
        if cand.lower() in lower:
            return lower[cand.lower()]
    return None

def iter_csv_examples(
    csv_path: str,
    question_field: str = None,
    answer_field: str = None,
    question_index: int = None,
    answer_index: int = None,
    encoding: str = "utf-8",
    delimiter: str = None,
    no_header: bool = False,
):
    """
    CSV: Question/Answer åˆ—ã‚’èª­ã¿è¾¼ã¿ã€‚
    - ãƒ˜ãƒƒãƒ€ãƒ¼æœ‰ â†’ DictReader
    - ãƒ˜ãƒƒãƒ€ãƒ¼ç„¡ â†’ readerï¼ˆåˆ—ç•ªå·ã§æŒ‡å®šã€æœªæŒ‡å®šãªã‚‰ 0,1 ã‚’æ¡ç”¨ï¼‰
    - åŒºåˆ‡ã‚Šæ–‡å­—ãƒ»ãƒ˜ãƒƒãƒ€ãƒ¼æœ‰ç„¡ã¯è‡ªå‹•æ¨å®šï¼ˆæ‰‹å‹•æŒ‡å®šãŒå„ªå…ˆï¼‰
    """
    # åŒºåˆ‡ã‚Š/ãƒ˜ãƒƒãƒ€ãƒ¼è‡ªå‹•æ¨å®šï¼ˆæœªæŒ‡å®šã®å ´åˆï¼‰
    if delimiter is None or no_header is None:
        auto_delim, auto_has_header = sniff_csv(csv_path, encoding=encoding)
        if delimiter is None:
            delimiter = auto_delim
        if no_header is None:
            no_header = not auto_has_header

    # æ–‡å­—åŒ–ã‘/BOMå¯¾ç­–ã¨ã—ã¦ utf-8-sig ã‚‚è©¦ã™
    try_encodings = [encoding]
    if encoding.lower() != "utf-8-sig":
        try_encodings.append("utf-8-sig")

    last_err = None
    for enc in try_encodings:
        try:
            with open(csv_path, "r", encoding=enc, newline="") as f:
                if not no_header:
                    reader = csv.DictReader(f, delimiter=delimiter)
                    if reader.fieldnames is None:
                        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åãŒå–ã‚Œãªã„å ´åˆã¯ç”Ÿèª­ã¿ã¸åˆ‡æ›¿
                        f.seek(0)
                        rdr = csv.reader(f, delimiter=delimiter)
                        for row in rdr:
                            if not row:
                                continue
                            q_idx = 0 if question_index is None else question_index
                            a_idx = 1 if answer_index is None else answer_index
                            if len(row) <= max(q_idx, a_idx):
                                continue
                            q = (row[q_idx] or "").strip()
                            a = (row[a_idx] or "").strip()
                            if q and a:
                                yield {"user": q, "assistant": a}
                        return

                    # åˆ—åã®è‡ªå‹•æ¨å®š
                    q_field = question_field or pick_field_by_candidates(reader.fieldnames, QUESTION_FIELD_CANDIDATES)
                    a_field = answer_field or pick_field_by_candidates(reader.fieldnames, ANSWER_FIELD_CANDIDATES)
                    if q_field is None or a_field is None:
                        raise ValueError(
                            f"CSVã®è³ªå•/å›ç­”ã‚«ãƒ©ãƒ ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚è¦‹ã¤ã‹ã£ãŸåˆ—: {reader.fieldnames}\n"
                            f"æ‰‹å‹•ã§ --question_field ã¨ --answer_field ã‚’æŒ‡å®šã™ã‚‹ã‹ã€--no_header ã¨åˆ—ç•ªå·æŒ‡å®šã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"
                        )

                    for row in reader:
                        q = (row.get(q_field) or "").strip()
                        a = (row.get(a_field) or "").strip()
                        if q and a:
                            yield {"user": q, "assistant": a}
                else:
                    rdr = csv.reader(f, delimiter=delimiter)
                    q_idx = 0 if question_index is None else question_index
                    a_idx = 1 if answer_index is None else answer_index
                    for row in rdr:
                        if not row:
                            continue
                        if len(row) <= max(q_idx, a_idx):
                            continue
                        q = (row[q_idx] or "").strip()
                        a = (row[a_idx] or "").strip()
                        if q and a:
                            yield {"user": q, "assistant": a}
            return
        except Exception as e:
            last_err = e
            continue
    # å…¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å¤±æ•—
    raise last_err if last_err else RuntimeError("CSVèª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

def make_out_paths(output_dir: str, name_for_out: str):
    os.makedirs(output_dir, exist_ok=True)
    output_pt    = os.path.join(output_dir, f"{name_for_out}.pt")
    output_jsonl = os.path.join(output_dir, f"{name_for_out}.jsonl")
    return output_pt, output_jsonl

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--source_type", type=str, choices=["hf", "csv"], default="hf",
                        help="å…¥åŠ›ã‚½ãƒ¼ã‚¹ã®ç¨®åˆ¥ï¼ˆhf: HuggingFace, csv: ãƒ­ãƒ¼ã‚«ãƒ«CSVï¼‰")

    # HF
    parser.add_argument("--dataset_name", type=str,
                        help="HuggingFaceä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåï¼ˆä¾‹: myorg/mydatasetï¼‰")

    # CSV
    parser.add_argument("--csv_path", type=str, help="Question/Answeråˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆ--source_type csv æ™‚ï¼‰")
    parser.add_argument("--question_field", type=str, default=None,
                        help="CSVã®è³ªå•ã‚«ãƒ©ãƒ åï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼æœ‰ã®å ´åˆï¼‰")
    parser.add_argument("--answer_field", type=str, default=None,
                        help="CSVã®å›ç­”ã‚«ãƒ©ãƒ åï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼æœ‰ã®å ´åˆï¼‰")
    parser.add_argument("--question_index", type=int, default=None,
                        help="ãƒ˜ãƒƒãƒ€ãƒ¼ç„¡ã®å ´åˆã®è³ªå•åˆ—ç•ªå·ï¼ˆ0å§‹ã¾ã‚Šã€‚æœªæŒ‡å®šãªã‚‰0ï¼‰")
    parser.add_argument("--answer_index", type=int, default=None,
                        help="ãƒ˜ãƒƒãƒ€ãƒ¼ç„¡ã®å ´åˆã®å›ç­”åˆ—ç•ªå·ï¼ˆ0å§‹ã¾ã‚Šã€‚æœªæŒ‡å®šãªã‚‰1ï¼‰")
    parser.add_argument("--delimiter", type=str, default=None,
                        help="CSVã®åŒºåˆ‡ã‚Šæ–‡å­—ï¼ˆæœªæŒ‡å®šãªã‚‰è‡ªå‹•æ¤œå‡ºï¼‰")
    parser.add_argument("--no_header", action="store_true",
                        help="CSVã«ãƒ˜ãƒƒãƒ€ãƒ¼ãŒç„¡ã„å ´åˆã«æŒ‡å®šï¼ˆæœªæŒ‡å®šãªã‚‰è‡ªå‹•åˆ¤å®šï¼‰")
    parser.add_argument("--encoding", type=str, default="utf-8",
                        help="CSVèª­ã¿è¾¼ã¿æ™‚ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæ—¢å®š: utf-8ã€‚è‡ªå‹•ã§ utf-8-sig ã‚‚è©¦è¡Œï¼‰")

    # å…±é€š
    parser.add_argument("--output_dir", type=str, default=DEFAULT_OUTPUT_DIR)
    parser.add_argument("--target_tokens", type=int, default=DEFAULT_TARGET_TOKENS,
                        help="åé›†ç›®æ¨™ã®ç´¯ç©ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆæ—¢å®š: 100,000,000ï¼‰")
    parser.add_argument("--max_samples", type=int, default=None,
                        help="æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆä»»æ„ãƒ»æ—©æœŸæ‰“ã¡åˆ‡ã‚Šç”¨ï¼‰")

    args = parser.parse_args()

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ™ãƒ¼ã‚¹ã‚’æ±ºå®š
    if args.source_type == "hf":
        if not args.dataset_name:
            raise ValueError("--dataset_name ãŒå¿…è¦ã§ã™ï¼ˆ--source_type hfï¼‰")
        safe_name = sanitize_filename(args.dataset_name)
        name_for_out = safe_name
    else:
        if not args.csv_path:
            raise ValueError("--csv_path ãŒå¿…è¦ã§ã™ï¼ˆ--source_type csvï¼‰")
        base = os.path.splitext(os.path.basename(args.csv_path))[0]
        name_for_out = sanitize_filename(base)

    output_pt, output_jsonl = make_out_paths(args.output_dir, name_for_out)

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

    # å…¥åŠ›ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’æº–å‚™
    if args.source_type == "hf":
        example_iter = iter_hf_examples(args.dataset_name)
        print("ğŸ” Hugging Face ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ User/Assistant å½¢å¼ã‚’åé›†ä¸­...")
    else:
        example_iter = iter_csv_examples(
            csv_path=args.csv_path,
            question_field=args.question_field,
            answer_field=args.answer_field,
            question_index=args.question_index,
            answer_index=args.answer_index,
            encoding=args.encoding,
            delimiter=args.delimiter,
            no_header=getattr(args, "no_header", None),  # None ã®å ´åˆã¯è‡ªå‹•åˆ¤å®š
        )
        print("ğŸ” CSV ã‹ã‚‰ User/Assistant å½¢å¼ã‚’åé›†ä¸­...")

    total_tokens = 0
    sample_count = 0
    records = []

    with open(output_jsonl, "w", encoding="utf-8") as jf:
        for ex in example_iter:
            messages = [
                {"role": "user", "content": ex["user"]},
                {"role": "assistant", "content": ex["assistant"]},
            ]
            try:
                tokens = tokenizer.apply_chat_template(messages, tokenize=True)
            except Exception as e:
                print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {e}")
                continue

            if not tokens:
                continue

            # 1è¡Œ1JSONï¼ˆå­¦ç¿’ç”¨ã«ä½¿ã„ã‚„ã™ã„ .jsonlï¼‰
            line = {
                "messages": messages,
                "token_count": len(tokens),
            }
            jf.write(json.dumps(line, ensure_ascii=False) + "\n")

            # .pt ç”¨ï¼ˆmessages ã®é…åˆ—ã®ã¿ã‚’æ ¼ç´ï¼‰
            records.append(messages)

            total_tokens += len(tokens)
            sample_count += 1

            if sample_count % 1000 == 0:
                print(f"ğŸª„ {sample_count} ä»¶ / ç´¯ç©ãƒˆãƒ¼ã‚¯ãƒ³æ•° {total_tokens:,}")

            if args.max_samples is not None and sample_count >= args.max_samples:
                print(f"ğŸ§¯ max_samples ã«åˆ°é”: {sample_count} ä»¶ã§åœæ­¢")
                break

            if args.target_tokens is not None and total_tokens >= args.target_tokens:
                print(f"ğŸ¯ ç›®æ¨™åˆ°é”: {total_tokens:,} ãƒˆãƒ¼ã‚¯ãƒ³")
                break

    print(f"ğŸ’¾ .pt ä¿å­˜ä¸­: {output_pt}")
    torch.save(records, output_pt, _use_new_zipfile_serialization=True)

    print("âœ… ä¿å­˜å®Œäº†:")
    print(f"    - {output_pt}")
    print(f"    - {output_jsonl}")

if __name__ == "__main__":
    main()
