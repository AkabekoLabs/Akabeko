import json
import os
import argparse
import csv
import torch
from datasets import load_dataset
from transformers import AutoTokenizer

MODEL_NAME = "openai/gpt-oss-20b"
DEFAULT_OUTPUT_DIR = "tokenized_dataset"
DEFAULT_TARGET_TOKENS = 100_000_000  # æ—¢å®šå€¤ã¯å¾“æ¥ã©ãŠã‚Š

def sanitize_filename(name: str) -> str:
    return name.replace("/", "__").replace("\\", "__")

def iter_hf_examples(dataset_name: str):
    # HF: instruction / input / output å½¢å¼ã‚’æƒ³å®š
    ds = load_dataset(dataset_name, split="train", streaming=True)
    for ex in ds:
        instruction = (ex.get("instruction") or "").strip()
        input_text  = (ex.get("input") or "").strip()
        output_text = (ex.get("output") or "").strip()
        if not instruction or not output_text:
            continue
        user_message = f"{instruction}\n{input_text}" if input_text else instruction
        yield {
            "user": user_message,
            "assistant": output_text
        }

def iter_csv_examples(csv_path: str, question_field: str, answer_field: str, encoding: str = "utf-8"):
    # CSV: Question / Answer åˆ—ã‚’æƒ³å®šï¼ˆåˆ—åã¯å¯å¤‰ï¼‰
    with open(csv_path, "r", encoding=encoding, newline="") as f:
        reader = csv.DictReader(f)
        if question_field not in reader.fieldnames or answer_field not in reader.fieldnames:
            raise ValueError(
                f"CSVã®åˆ—åãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚è¦‹ã¤ã‹ã£ãŸåˆ—: {reader.fieldnames} / "
                f"æœŸå¾…: '{question_field}', '{answer_field}'"
            )
        for row in reader:
            q = (row.get(question_field) or "").strip()
            a = (row.get(answer_field) or "").strip()
            if not q or not a:
                continue
            yield {
                "user": q,
                "assistant": a
            }

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--source_type", type=str, choices=["hf", "csv"], default="hf",
                        help="å…¥åŠ›ã‚½ãƒ¼ã‚¹ã®ç¨®åˆ¥ï¼ˆhf: HuggingFace, csv: ãƒ­ãƒ¼ã‚«ãƒ«CSVï¼‰")
    parser.add_argument("--dataset_name", type=str,
                        help="HuggingFaceä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåï¼ˆä¾‹: myorg/mydatasetï¼‰")
    parser.add_argument("--csv_path", type=str, help="Question/Answeråˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆ--source_type csv æ™‚ï¼‰")
    parser.add_argument("--question_field", type=str, default="Question",
                        help="CSVã®è³ªå•ã‚«ãƒ©ãƒ åï¼ˆæ—¢å®š: Questionï¼‰")
    parser.add_argument("--answer_field", type=str, default="Answer",
                        help="CSVã®å›ç­”ã‚«ãƒ©ãƒ åï¼ˆæ—¢å®š: Answerï¼‰")
    parser.add_argument("--output_dir", type=str, default=DEFAULT_OUTPUT_DIR)
    parser.add_argument("--target_tokens", type=int, default=DEFAULT_TARGET_TOKENS,
                        help="åé›†ç›®æ¨™ã®ç´¯ç©ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆæ—¢å®š: 100,000,000ï¼‰")
    parser.add_argument("--max_samples", type=int, default=None,
                        help="æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆä»»æ„ãƒ»æ—©æœŸæ‰“ã¡åˆ‡ã‚Šç”¨ï¼‰")
    parser.add_argument("--encoding", type=str, default="utf-8",
                        help="CSVèª­ã¿è¾¼ã¿æ™‚ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæ—¢å®š: utf-8ï¼‰")
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ™ãƒ¼ã‚¹ã‚’æ±ºå®š
    if args.source_type == "hf":
        if not args.dataset_name:
            raise ValueError("--dataset_name ãŒå¿…è¦ã§ã™ï¼ˆ--source_type hfï¼‰")
        safe_name = sanitize_filename(args.dataset_name)
        name_for_out = safe_name
    else:
        if not args.csv_path:
            raise ValueError("--csv_path ãŒå¿…è¦ã§ã™ï¼ˆ--source_type csvï¼‰")
        base = os.path.splitext(os.path.basename(args.csv_path))[0]
        name_for_out = sanitize_filename(base)

    output_pt  = os.path.join(args.output_dir, f"{name_for_out}.pt")
    output_txt = os.path.join(args.output_dir, f"{name_for_out}.txt")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

    # å…¥åŠ›ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’æº–å‚™
    if args.source_type == "hf":
        example_iter = iter_hf_examples(args.dataset_name)
        print("ğŸ” Hugging Face ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ User/Assistant å½¢å¼ã‚’åé›†ä¸­...")
    else:
        example_iter = iter_csv_examples(args.csv_path, args.question_field, args.answer_field, encoding=args.encoding)
        print("ğŸ” CSVï¼ˆQuestion/Answerï¼‰ã‹ã‚‰ User/Assistant å½¢å¼ã‚’åé›†ä¸­...")

    total_tokens = 0
    sample_count = 0
    records = []

    with open(output_txt, "w", encoding="utf-8") as txt_file:
        for ex in example_iter:
            messages = [
                {"role": "user", "content": ex["user"]},
                {"role": "assistant", "content": ex["assistant"]},
            ]
            try:
                tokens = tokenizer.apply_chat_template(messages, tokenize=True)
            except Exception as e:
                print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {e}")
                continue

            if not tokens:
                continue

            # 1è¡Œ1JSON ã‚’è¿½è¨˜
            line = {
                "messages": messages,
                "token_count": len(tokens),
            }
            txt_file.write(json.dumps(line, ensure_ascii=False) + "\n")

            # .pt ç”¨ã«ãƒ¡ãƒ¢ãƒªã¸æ ¼ç´
            records.append(messages)

            total_tokens += len(tokens)
            sample_count += 1

            if sample_count % 1000 == 0:
                print(f"ğŸª„ {sample_count} ä»¶ / ç´¯ç©ãƒˆãƒ¼ã‚¯ãƒ³æ•° {total_tokens:,}")

            if args.max_samples is not None and sample_count >= args.max_samples:
                print(f"ğŸ§¯ max_samples ã«åˆ°é”: {sample_count} ä»¶ã§åœæ­¢")
                break

            if total_tokens >= args.target_tokens:
                print(f"ğŸ¯ ç›®æ¨™åˆ°é”: {total_tokens:,} ãƒˆãƒ¼ã‚¯ãƒ³")
                break

    print(f"ğŸ’¾ .pt ä¿å­˜ä¸­: {output_pt}")
    torch.save(records, output_pt, _use_new_zipfile_serialization=True)

    print("âœ… ä¿å­˜å®Œäº†:")
    print(f"    - {output_pt}")
    print(f"    - {output_txt}")

if __name__ == "__main__":
    main()