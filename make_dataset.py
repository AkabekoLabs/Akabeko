import json
import torch
from datasets import load_dataset
from transformers import AutoTokenizer
import os
import argparse

MODEL_NAME = "Qwen/Qwen3-0.6B"
TARGET_TOKEN_COUNT = 100_000_000
CONFIG_NAME = "jpn_Jpan"
DEFAULT_OUTPUT_DIR = "tokenized_dataset"

def sanitize_filename(name):
    return name.replace("/", "__")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset_name", type=str, required=True, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ™ãƒ¼ã‚¹")
    parser.add_argument("--output_dir", type=str, default=DEFAULT_OUTPUT_DIR)
    args = parser.parse_args()

    dataset_name = args.dataset_name
    output_dir = args.output_dir
    safe_name = sanitize_filename(dataset_name)  # â† ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»

    os.makedirs(output_dir, exist_ok=True)

    output_pt = os.path.join(output_dir, f"{safe_name}.pt")
    output_bin = os.path.join(output_dir, f"{safe_name}.bin")
    output_txt = os.path.join(output_dir, f"{safe_name}.txt")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    dataset = load_dataset(dataset_name, CONFIG_NAME, split="train", streaming=True)

    total_tokens = 0
    sample_count = 0
    records = []

    print("ğŸ” æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’åé›†ä¸­...")

    with open(output_txt, "w", encoding="utf-8") as txt_file:
        for example in dataset:
            text = example.get("text", "").strip()
            if not text:
                continue

            tokens = tokenizer.encode(text, add_special_tokens=False)
            token_count = len(tokens)

            if token_count == 0:
                continue

            record = {
                "text": text,
                "input_ids": torch.tensor(tokens, dtype=torch.long)
            }
            records.append(record)

            txt_file.write(json.dumps({
                "text": text,
                "input_ids": tokens
            }, ensure_ascii=False) + "\n")

            total_tokens += token_count
            sample_count += 1

            if sample_count % 1000 == 0:
                print(f"ğŸª„ {sample_count} ä»¶ç›®: ç´¯ç©ãƒˆãƒ¼ã‚¯ãƒ³æ•° {total_tokens:,}")

            if total_tokens >= TARGET_TOKEN_COUNT:
                print(f"ğŸ¯ ç›®æ¨™åˆ°é”: {total_tokens:,} ãƒˆãƒ¼ã‚¯ãƒ³")
                break

    print(f"ğŸ’¾ .pt ä¿å­˜ä¸­: {output_pt}")
    torch.save(records, output_pt, _use_new_zipfile_serialization=True)

    print(f"ğŸ’¾ .bin ä¿å­˜ä¸­: {output_bin}")
    torch.save(records, output_bin, _use_new_zipfile_serialization=True)

    print(f"âœ… ä¿å­˜å®Œäº†:")
    print(f"    - {output_pt}")
    print(f"    - {output_bin}")
    print(f"    - {output_txt}")

if __name__ == "__main__":
    main()

